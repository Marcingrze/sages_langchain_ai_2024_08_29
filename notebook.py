# -*- coding: utf-8 -*-
"""sages 29_30_08_2024

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kCUFXApP2hHJl5XT8sbqVvUhyNyteBoM
"""

!pip install openai langchain langchain-openai langserve[all] chromadb langchain-chroma langchain-community langgraph

API_KEY = "sk-proj-..."
MODEL_ID = "gpt-4o-mini"

"""## Completions API"""

from openai import OpenAI
client = OpenAI(api_key=API_KEY)

response = client.completions.create(
    model="gpt-3.5-turbo-instruct",
    prompt="Finish the sentence: \"Litwo, Ojczyzno moja\""
)
response.choices[0].text

response = client.completions.create(
    model="gpt-3.5-turbo-instruct",
    prompt="Finish the sentence: \"Litwo, Ojczyzno moja\"",
    temperature=1.2
)
response.choices[0].text

response = client.completions.create(
    model="gpt-3.5-turbo-instruct",
    prompt="Finish the sentence: \"Litwo, Ojczyzno moja\"",
    temperature=0.0
)
response.choices[0].text

response = client.completions.create(
    model="gpt-3.5-turbo-instruct",
    prompt="Finish the sentence: \"Litwo, Ojczyzno moja\"",
    max_tokens=100,
    temperature=0.0
)
response

response = client.completions.create(
    model="gpt-3.5-turbo-instruct",
    prompt="Finish the sentence: \"Litwo, Ojczyzno moja\"",
    max_tokens=100,
    temperature=1.2,
    n=3
)
response

for choice in response.choices:
    print(choice.text)

response = client.completions.create(
    model="gpt-3.5-turbo-instruct",
    prompt="Finish the sentence: \"Litwo, Ojczyzno moja\"",
    max_tokens=100,
    temperature=1.2,
    stop="."
)
response

"""## ChatCompletion"""

response = client.chat.completions.create(
    model=MODEL_ID,
    messages=[
        {"role": "user", "content": "Dokończ zdanie: \"Litwo, Ojczyzno moja\""}
    ]
)
response

response = client.chat.completions.create(
    model=MODEL_ID,
    messages=[
        {"role": "system", "content": "Dokończ wiersz używając słowa \"wrona\" w każdej linii"},
        {"role": "user", "content": "Litwo, Ojczyzno moja"}
    ]
)
response

response = client.chat.completions.create(
    model=MODEL_ID,
    messages=[
        {"role": "system", "content": "Napisz dowolne słowo zaczynające się na tą samą literę co trzecie słowo w podanym zdaniu."},
        {"role": "user", "content": "Litwo, Ojczyzno moja"},
        {"role": "assistant", "content": "Trzecie słowo: \"moja\", pierwsza litera: \"m\", odpowiedź: \"Mikrofalówka\""},
        {"role": "user", "content": "Koń jaki jest każdy widzi"}
    ]
)
response

response = client.chat.completions.create(
    model=MODEL_ID,
    messages=[
        {"role": "system", "content": "Dokończ wiersz używając słowa \"wrona\" w każdej linii"},
        {"role": "user", "content": "Litwo, Ojczyzno moja"}
    ],
    n=2,
    max_tokens=100
)
response

for choice in response.choices:
  print(choice.message.content)
  print('---')

"""## Generowanie JSON"""

response = client.chat.completions.create(
    model=MODEL_ID,
    response_format={"type": "json_object"},
    messages=[
        {"role": "system", "content": "What is the third word in the sentence. Return a JSON object with the key 'third_word'"},
        {"role": "user", "content": "Litwo, Ojczyzno moja"}
    ]
)
response

from typing import List
from pydantic import BaseModel

class NamedEntity(BaseModel):
  entity_type: str
  value: str

class NamedEntityRecognition(BaseModel):
  entities: List[NamedEntity]

completion = client.beta.chat.completions.parse(
    model="gpt-4o-2024-08-06",
    messages=[
        {"role": "system", "content": "As a Named Entity Recognition system find entities mentioned in the given text. Supported types: PERSON_NAME, LOCATION_CITY, LOCATION_COUNTRY, LOCATION_PLANET."},
        {"role": "user", "content": "Emma Watson attended the film premiere in London, UK, Earth last night."}
    ],
    response_format=NamedEntityRecognition
)

completion

completion.choices[0].message.parsed

"""## OpenAI Functions"""

import json

def fake_function(name, department):
  print(f"User: {name}, Department: {department}")

  data = {
      "name": name,
      "department": department,
      "date_of_birth": "1990-01-01",
      "position": f"Head of {department}"
  }

  return json.dumps(data)

fake_function("John", "HR")

functions = [
{
    "type": "function",
    "function": {
        "name": "find_user_data",
        "description": "Retrieves user's data from the database by a given name and department",
        "parameters": {
            "type": "object",
            "properties": {
                "name": {
                    "type": "string",
                    "description": "The name of the user",
                },
                "department": {
                    "type": "string",
                    "description": "The department's name",
                    "enum": ["IT", "HR", "LEGAL", "MARKETING"],
                }
            },
            "required": ["name", "department"],
        },
    }
}
]

system_messages = [
    {"role": "system", "content": "You are an HR helper who makes database queries on behalf of an HR representative"},
    {"role": "system", "content": "You can retrieve employee's data from a database by the person's name and department's name. The response is a JSON string with all available data."},
    {"role": "system", "content": "Current date: 2024-05-01"},
]

messages = system_messages + [{"role": "user", "content": "How old is John Smith from the legal department?"}]

response = client.chat.completions.create(
    model="gpt-4o",
    messages=messages,
    tools=functions
)

response

ai_msg = response.choices[0].message
for tool_call in ai_msg.tool_calls:
  function_args = json.loads(tool_call.function.arguments)
  print(tool_call.function.name)
  print(function_args)
  if tool_call.function.name == "find_user_data":
    result = fake_function(**function_args)
    print(result)

function_response_msg = {
    "tool_call_id": ai_msg.tool_calls[0].id,
    "role": "tool",
    "name": "find_user_data",
    "content": result
}

messages.append(ai_msg)
messages.append(function_response_msg)
messages

response = client.chat.completions.create(
    model="gpt-4o",
    messages=messages,
    tools=functions
)

response

"""## OpenAI Functions - REST API"""

import requests

def call_rest_api(arguments):
    url = 'https://reqres.in' + arguments['url']
    body = arguments.get('body', {})
    response = None
    if arguments['method'] == 'GET':
        response = requests.get(url)
    elif arguments['method'] == 'POST':
        response = requests.post(url, json=body)
    elif arguments['method'] == 'PUT':
        response = requests.put(url, json=body)
    elif arguments['method'] == 'DELETE':
        response = requests.delete(url)
    else:
        raise ValueError(arguments)

    if response.status_code == 200:
        return json.dumps(response.json())
    else:
        return f"Status code: {response.status_code}"

functions = [
{
    "type": "function",
    "function": {
        "name": "call_rest_api",
        "description": "Sends a request to the REST API",
        "parameters": {
            "type": "object",
            "properties": {
                "method": {
                    "type": "string",
                    "description": "The HTTP method to be used",
                    "enum": ["GET", "POST", "PUT", "DELETE"],
                },
                "url": {
                    "type": "string",
                    "description": "The URL of the endpoint (without host! For example: '/api/users'). One of the URL values from API description. Value placeholders must be replaced with actual values.",
                },
                "body": {
                    "type": "string",
                    "description": "A string representation of the JSON that should be sent as the request body.",
                },

            },
            "required": ["method", "url"],
        },
    }
}
]

available_apis = [
        {'method': 'GET', 'url': '/api/users?page=[page_id]', 'description': 'Lists employees. The response is paginated. You may need to request more than one to get them all. For example,/api/users?page=2.'},
        {'method': 'GET', 'url': '/api/users/[user_id]', 'description': 'Returns information about the employee identified by the given id. For example,/api/users/2'},
        {'method': 'POST', 'url': '/api/users', 'description': 'Creates a new employee profile. This function accepts JSON body containing two fields: name and job'},
        {'method': 'PUT', 'url': '/api/users/[user_id]', 'description': 'Updates employee information. This function accepts JSON body containing two fields: name and job. The user_id in the URL must be a valid identifier of an existing employee.'},
        {'method': 'DELETE', 'url': '/api/users/[user_id]', 'description': 'Removes the employee identified by the given id. Before you call this function, find the employee information and make sure the id is correct. Do NOT call this function if you didn\'t retrieve user info. Iterate over all pages until you find it or make sure it doesn\'t exist'}
    ]

import json

messages = [
    {"role": "user", "content": "You are an HR helper who makes API calls on behalf of an HR representative"},
    {"role": "user", "content": "You have access to the following APIs: " + json.dumps(available_apis)},
    {"role": "user", "content": "If a function requires an identifier, list all employees first to find the proper value. You may need to list more than one page"},
    {"role": "user", "content": "If you were asked to create, update, or delete a user, perform the action and reply with a confirmation telling what you have done."}
]

def call_ai(new_message):
    if new_message:
        messages.append({"role": "user", "content": new_message})

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        tools=functions,
        tool_choice="auto",
    )

    msg = response.choices[0].message
    messages.append(msg)
    if msg.content:
        print(msg.content)
    if msg.tool_calls:
        for tool_call in msg.tool_calls:
            function_name = tool_call.function.name
            function_args = json.loads(tool_call.function.arguments)
            if function_name == 'call_rest_api':
                print(function_args)
                function_response = call_rest_api(function_args)
                print(function_response)
                messages.append({
                    "tool_call_id": tool_call.id,
                    "role": "tool",
                    "name": function_name,
                    "content": function_response
                })

        call_ai(new_message=None)
    else:
      return msg

call_ai("Fire user with the id 7")

"""## ZADANIE

Dostęp do CRM - dane klientów

1. Chcemy pobierać adres e-mail klienta na podstawie imienia i nazwiska.
2. Chcemy pobrać wszystkie adresy e-mail.
"""

functions = [
{
    "type": "function",
    "function": {
        "name": "call_rest_api",
        "description": "Sends a request to the REST API",
        "parameters": {
            "type": "object",
            "properties": {
                "url": {
                    "type": "string",
                    "description": "The URL of the endpoint (without host! For example: '/api/users'). One of the URL values from API description. Value placeholders must be replaced with actual values.",
                },

            },
            "required": ["url"],
        },
    }
}
]

import requests

def call_rest_api(arguments):
    url = 'https://reqres.in' + arguments['url']

    response = requests.get(url)
    if response.status_code == 200:
        return json.dumps(response.json())
    else:
        return f"Status code: {response.status_code}"

available_apis = [
        {'method': 'GET', 'url': '/api/users?page=[page_id]', 'description': 'Lists clients. The response is paginated. You may need to request more than one to get them all. For example,/api/users?page=2.'},
        {'method': 'GET', 'url': '/api/users/[user_id]', 'description': 'Returns information about the client identified by the given id. For example,/api/users/2'}
    ]

messages = [
    {"role": "user", "content": "You are an AI agent with access to the CRM"},
    {"role": "user", "content": "You have access to the following APIs: " + json.dumps(available_apis)},
    {"role": "user", "content": "If you are asked to retrieve client data by name, list all pages of client data until you find the required value"},
]

call_ai("Give me email addresses of all users")

messages = [
    {"role": "user", "content": "You are an AI agent with access to the CRM"},
    {"role": "user", "content": "You have access to the following APIs: " + json.dumps(available_apis)},
    {"role": "user", "content": "If you are asked to retrieve client data by name, list all pages of client data until you find the required value"},
]

call_ai("Find Lindsay Ferguson's email")

"""## Langchain"""

import os

os.environ["LANGCHAIN_TRACING_V2"]="true"
os.environ["LANGCHAIN_ENDPOINT"]="https://api.smith.langchain.com"
os.environ["LANGCHAIN_API_KEY"]="lsv2_pt_..."
os.environ["LANGCHAIN_PROJECT"]="sages_29_30_08"

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

model = ChatOpenAI(openai_api_key=API_KEY, model=MODEL_ID)

prompt = ChatPromptTemplate.from_template("Napisz definicję słowa w sposób zrozumiały dla przedszkolaka: {word}")
chain = prompt | model

chain.input_schema.schema()

chain.invoke({"word": "koń"})

chain.batch([{"word": "kot"}, {"word": "klej"}], config={"max_concurrency": 5})

for s in chain.stream({"word": "skakanka"}):
  print(s.content, end="", flush=True)

from langchain_core.output_parsers import StrOutputParser

chain = prompt | model | StrOutputParser()
chain.invoke({"word": "Bojowy Wóz Piechoty"})

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "Napisz definicję słowa w sposób zrozumiały dla przedszkolaka"),
        ("human", "Co to jest {word}?")
    ]
)

chain = prompt | model | StrOutputParser()
chain.invoke({"word": "Drapak dla kota"})

from langchain_core.runnables import RunnablePassthrough
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "Napisz definicję słowa w sposób zrozumiały dla przedszkolaka"),
        ("human", "Co to jest {word}?")
    ]
)

chain = ({"word": RunnablePassthrough()} | prompt) | model | StrOutputParser()

chain.input_schema.schema()

chain.invoke("Dinozaur")

"""## ChatHistory"""

from langchain.memory import ChatMessageHistory

history = ChatMessageHistory()

history.add_user_message("Co to jest Chomik?")
history.add_ai_message("Chomik to małe zwierzątko, które mieszka w specjalnej klatce w domu. Chomiki mają miękkie futerko, duże oczy i puszysty ogonek. Lubią biegać w kołowrotku, jeść ziarna i spać w swojej gniazdku. Chomiki są bardzo urocze i łatwe do opieki.")

history.messages

from langchain_core.prompts import MessagesPlaceholder

prompt = ChatPromptTemplate.from_messages([
    ("system", "Napisz definicję słowa w sposób zrozumiały dla przedszkolaka"),
    MessagesPlaceholder(variable_name="messages"),
    ("human", "Co to jest {word}?")
])

chain = prompt | model | StrOutputParser()

chain.invoke({"messages": history.messages, "word": "Krzesło"})

history.messages

"""## Działająca historia"""

from langchain_core.runnables.history import RunnableWithMessageHistory

history_dict = {}

def get_history(session_id):
  if session_id not in history_dict:
    history_dict[session_id] = ChatMessageHistory()
  return history_dict[session_id]

chain_with_memory = RunnableWithMessageHistory(
    chain,
    get_history,
    input_messages_key="word",
    history_messages_key="messages"
)

history_dict

chain_with_memory.invoke({"word": "Smok Wawelski"}, {"configurable": {"session_id": "1"}})

history_dict

chain_with_memory.invoke({"word": "Lodówka"}, {"configurable": {"session_id": "1"}})

history_dict

"""## RAG"""

from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://mikulskibartosz.name/cupid-principles-in-data-engineering")
data = loader.load()
data

from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
all_splits = text_splitter.split_documents(data)
all_splits

from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(openai_api_key=API_KEY)
vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)

retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

docs = retriever.invoke("How to use unix philosophy in data engineering?")

docs

from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

SYSTEM_TEMPLATE = """
Answer the user's questions based on the below context.
If the context doesn't contain any relevant information to the question, don't make something up and just say "I don't know":

<context>
{context}
</context>
"""

question_answering_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", SYSTEM_TEMPLATE),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

document_chain = create_stuff_documents_chain(model, question_answering_prompt)

from langchain_core.messages import HumanMessage

document_chain.invoke(
    {
        "context": docs,
        "messages": [
            HumanMessage(content="How to use unix philosophy in data engineering?")
        ],
    }
)

template = """Answer the question based only on the following context:

{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)


def format_docs(docs):
  return "\n\n".join(doc.page_content for doc in docs)

chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

print(chain.invoke("What is CUPID?"))

"""## ZADANIE 2

Użyj YoutubeLoader do zaimplementowania Langchain chain odpowiadającego na pytania dot. https://www.youtube.com/watch?v=U9mJuUkhUzk
"""

!pip install youtube-transcript-api

from langchain_community.document_loaders import YoutubeLoader
yt_loader = YoutubeLoader("U9mJuUkhUzk")
yt_data = yt_loader.load()
yt_data

from langchain_text_splitters import RecursiveCharacterTextSplitter
yt_text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
yt_all_splits = yt_text_splitter.split_documents(yt_data)
yt_all_splits

yt_embeddings = OpenAIEmbeddings(openai_api_key=API_KEY)
yt_vectorstore = Chroma.from_documents(documents=yt_all_splits, embedding=yt_embeddings)

yt_retriever = yt_vectorstore.as_retriever(search_kwargs={"k": 2})

template = """Answer the question based only on the following context:
{context}
Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

def yt_format_docs(docs):
  return "\n\n".join(doc.page_content for doc in docs)

chain = (
  {"context": yt_retriever | yt_format_docs, "question": RunnablePassthrough()}
  | prompt
  | model
  | StrOutputParser()
)
print(chain.invoke("How to use voice with AI?"))

"""https://mikulskibartosz.name/advanced-rag-techniques-explained

## Langchain Agent
"""

from langchain.agents import initialize_agent, Tool, AgentType
from langchain.tools.base import StructuredTool

def call_rest_api(method: str, url: str) -> dict:
  """Sends a request to the REST API.

  Accepts two parameters:
  * method - The HTTP method to be used (GET, POST, PUT, or DELETE)
  * url - The URL of the endpoint. Value placeholders must be replaced with actual values.

  For example: GET /api/users?page=1 or DELETE /api/users/13

  To find users by name, use the GET method first.

  Available API endpoints:
  'api': 'GET /api/users?page=[page_id]', 'description': 'Lists employees. The response is paginated. You may need to request more than one to get them all. For example,/api/users?page=2.'
  'api': 'DELETE /api/users/[numeric user_id]', 'description': 'Removes the employee identified by the given id. Before you call this function, find the employee information and make sure the id is correct. Do NOT call this function if you didn\'
  """
  url = 'https://reqres.in' + url
  response = None
  if method == 'GET':
      response = requests.get(url)
  elif method == 'DELETE':
      response = requests.delete(url)
  else:
      raise ValueError(method)

  if response.status_code == 200:
      return response.json()
  else:
      return f"Status code: {response.status_code}"

tools = [
    StructuredTool.from_function(call_rest_api)
]

agent = initialize_agent(tools, model, agent=AgentType.OPENAI_FUNCTIONS, verbose=True)

agent.run("Find a list of all emails")

"""## Langchain Agent z historią"""

from langchain.agents import create_tool_calling_agent
from langchain.agents import AgentExecutor
from langchain.memory import ConversationBufferMemory


prompt = ChatPromptTemplate.from_messages([
    ("system", "You are an HR helper who makes API calls on behalf of an HR representative"),
    ("system", "You have access to the following APIs: " + json.dumps(available_apis).replace("{", "{{").replace("}", "}}")),
    ("system", "If a function requires an identifier, list all employees first to find the proper value. You may need to list more than one page"),
    ("system", "If you were asked to create, update, or delete a user, perform the action and reply with a confirmation telling what you have done."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])

memory = ConversationBufferMemory(memory_key="chat_history")

agent = create_tool_calling_agent(model, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, max_iterations=4, verbose=True)

agent_with_message_history = RunnableWithMessageHistory(
    agent_executor,
    lambda session_id: memory.chat_memory, # tutaj powinniśmy wczytywać historię z bazy danych
    input_messages_key="input",
    history_messages_key="chat_history"
)

agent_with_message_history.invoke(
    {"input": "Count the number of employees"},
    config={"configurable": {"session_id": "123"}}
)

memory

"""## ZADANIE 3

Przygotuj Agenta Langchain z historią mającego dostęp do danych z pliku pdf znajdujących się w wektorowej bazie danych

https://api.sages.pl/content/trainings/sztuczna-inteligencja-ai-i-data-science/ai-openai-i-langchain.pdf
"""

!pip install pypdf

from langchain_community.document_loaders import PyPDFLoader

!wget https://api.sages.pl/content/trainings/sztuczna-inteligencja-ai-i-data-science/ai-openai-i-langchain.pdf

loader = PyPDFLoader("/content/ai-openai-i-langchain.pdf")
pages = loader.load_and_split()
pages

embeddings = OpenAIEmbeddings(openai_api_key=API_KEY)
vectorstore = Chroma.from_documents(documents=pages, embedding=embeddings)

retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

template = """Answer the question based only on the following context:

{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)
model = ChatOpenAI(openai_api_key=API_KEY)


def format_docs(docs):
    return "\n\n".join([d.page_content for d in docs])


chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

chain.invoke("Jaki jest czas trwania szkolenia?")

def search_in_documents(question: str) -> str:
  """Searches for an answer in the vector database.

  Accepts one parameter:
  * question - the user's question
  """
  return chain.invoke(question)

tools = [
    StructuredTool.from_function(search_in_documents)
]

prompt = ChatPromptTemplate.from_messages([
    ("system", """You are an AI agent with access to a vector database with information about programming workshops.

    The documents are in Polish and must be searched using Polish as the query language. Given the user's query, translate it to Polish (if required) and pass as the question to the search function.

    You must always pass the question to the search function.
    Return the answer in the same language as the user's question. If the user asked the question in English, translate the answer to English"""),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])

memory = ConversationBufferMemory(memory_key="chat_history")

agent = create_tool_calling_agent(model, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, max_iterations=4, verbose=True)

agent_with_message_history = RunnableWithMessageHistory(
    agent_executor,
    lambda session_id: memory.chat_memory, # tutaj powinniśmy wczytywać historię z bazy danych
    input_messages_key="input",
    history_messages_key="chat_history"
)

agent_with_message_history.invoke(
    {"input": "How long is the training?"},
    config={"configurable": {"session_id": "123"}}
)

agent_with_message_history.invoke(
    {"input": "What are the requirements for the participants?"},
    config={"configurable": {"session_id": "123"}}
)

"""## Marvin"""

!pip install marvin

import marvin
marvin.settings.openai.api_key=API_KEY

marvin.classify(
  """one of the worst fairprice i’ve been too tbh. kinda under staff and the weigh machine for price stickers to be printer aren’t up to date so you’ll need to go to the cashier to get the price tag instead""",
  labels=["positive", "negative"]
)

marvin.settings

marvin.extract(
    """one of the worst fairprice i’ve been too tbh. kinda under staff and the weigh machine for price stickers to be printer aren’t up to date so you’ll need to go to the cashier to get the price tag instead""",
    instructions="get any negative comments about employees"
)

names = marvin.generate(n=20, instructions="Polish first names and last names")
names

review = marvin.generate(n=1, instructions="a positive review of a restaurant but mention the owner's name")
review

class Person(BaseModel):
  first_name: str
  last_name: str

marvin.cast(review[0], target=Person)

@marvin.fn
def classify(text: str) -> str:
  """
  Classifies a given review as positive or negative.
  """

classify(review[0])

image = marvin.paint("A dog jumping over a cat")

image

image.data[0].url

@marvin.fn
def classify(text: str) -> str:
  """
  Classifies competition's reviews. For us, a positive review is negative, but a negative review is positive.

  Returns either "positive" or "negative" with no additional comments.
  """

classify(review[0])

"""## Open Source Models

Runtime: T4 GPU


"""

!pip install langchain-huggingface

os.environ["HF_TOKEN"] = "hf_..."

from langchain_huggingface import HuggingFacePipeline

llm = HuggingFacePipeline.from_model_id(
    model_id="HuggingFaceTB/SmolLM-1.7B",
    task="text-generation",
    device=0,
    pipeline_kwargs=dict(
        max_new_tokens=128,
        do_sample=True,
    ),
)

llm.invoke("This is a test")

prompt = ChatPromptTemplate.from_template("Explain the word to a preschooler: {word}")

chain = prompt | llm | StrOutputParser()
chain.invoke({"word": "Cat"})

"""## LangGraph"""

!wget https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv

import pandas as pd

df = pd.read_csv("titanic.csv")
df

survivors = df[["PassengerId", "Survived"]]
tickets = df[["PassengerId", "Ticket", "Pclass", "Fare", "Cabin", "Embarked"]]
passengers = df[["PassengerId", "Name", "Sex", "Age"]]

import sqlite3

con = sqlite3.connect("titanic.db")
survivors.to_sql("survivors", con, index=False, if_exists="replace")
tickets.to_sql("tickets", con, index=False, if_exists="replace")
passengers.to_sql("passengers", con, index=False, if_exists="replace")

def run_query(sql_query):
  con = sqlite3.connect("titanic.db")
  try:
    response = pd.read_sql_query(sql_query, con)
    return response.to_markdown()
  except Exception as e:
    return str(e)
  finally:
    con.close()

print(run_query("SELECT * FROM passengers LIMIT 5"))

"""```
                 +-----------+
                 | __start__ |
                 +-----------+
                       *
                       *
                       *
       +------------------------------+
       | check_if_can_answer_question |
       +------------------------------+
               ...            ...
             ..                  ..
           ..                      ..
 +-------------+                     ..
 | write_query |                      .
 +-------------+                      .
        *                             .
        *                             .
        *                             .
+---------------+                     .
| execute_query |                     .
+---------------+                     .
        *                             .
        *                             .
        *                             .
+--------------+            +-------------------+
| write_answer |            | explain_no_answer |
+--------------+            +-------------------+
               ***            ***
                  **        **
                    **    **
                  +---------+
                  | __end__ |
                  +---------+
```
"""

DB_DESCRIPTION = """
* "survivors" Table:
Columns:
PassengerId: A unique identifier for each passenger.
Survived: Indicates whether the passenger survived the sinking of the Titanic (typically coded as 1 for survived and 0 for did not survive).

* "tickets" Table:
Columns:
PassengerId: A unique identifier for each passenger, linking to the survivors table.
Ticket: The ticket number associated with each passenger.
Pclass: The passenger class; reflects the socio-economic status of the passenger (values: 1, 2, 3).
Fare: The amount of money paid for the ticket.
Cabin: The cabin number where the passenger stayed.
Embarked: The port at which the passenger embarked (C = Cherbourg; Q = Queenstown; S = Southampton).

* "passengers" Table:
Columns:
PassengerId: A unique identifier for each passenger, linking to the survivors table.
Name: The full name of the passenger.
Sex: The gender of the passenger.
Age: The age of the passenger at the time of the Titanic's voyage."""

"""### Definiujemy obiekt zawierający stan"""

from typing_extensions import TypedDict

class WorkflowState(TypedDict):
  question: str
  plan: str
  can_answer: bool
  sql_query: str
  sql_result: str
  answer: str

from langchain_core.output_parsers import JsonOutputParser

from langchain.prompts import PromptTemplate

model = ChatOpenAI(openai_api_key=API_KEY, model="gpt-4o")

can_answer_prompt = PromptTemplate(
    template="""You are a database reading bot that can answer users' questions using information from a database. \n

    {data_description} \n\n

    Given the user's question, decide whether the question can be answered using the information in the database. \n\n

    Return a JSON with two keys, 'reasoning' and 'can_answer', and no preamble or explanation.
    Return one of the following JSON:

    {{"reasoning": "I can find the number of women who survived by joining the table survivors with passengers and filtering by the Sex and Survived columns", "can_answer":true}}
    {{"reasoning": "I can find the number of passengers in each class by grouping the data from tickets table by the Pclass column and counting the number of rows in each group", "can_answer":true}}
    {{"reasoning": "I can't answer how many people traveled in a cabin on the top deck because there is no deck level information.", "can_answer":false}}

    Question: {question} \n
    """,
    input_variables=["data_description", "question"]
)

can_answer_chain = can_answer_prompt | model | JsonOutputParser()

can_answer_chain.invoke({"question": "How many people over age 80 died in the accident?", "data_description": DB_DESCRIPTION})

can_answer_chain.invoke({"question": "How many French people were on board?", "data_description": DB_DESCRIPTION})

def check_if_can_answer_question(state):
  result = can_answer_chain.invoke({"question": state["question"], "data_description": DB_DESCRIPTION})

  return {"plan": result["reasoning"], "can_answer": result["can_answer"]}

write_query_prompt = PromptTemplate(
    template="""You are a database reading bot that can answer users' questions using information from a database. \n

    {data_description} \n\n

    In the previous step, you have prepared the following plan: {plan}

    Return an SQL query with no preamble or explanation. Don't include any markdown characters or quotation marks around the query.

    Question: {question} \n""",
    input_variables=["data_description", "question", "plan"],
)

write_query_chain = write_query_prompt | model | StrOutputParser()

def write_query(state):
  result = write_query_chain.invoke({
      "data_description": DB_DESCRIPTION,
      "question": state["question"],
      "plan": state["plan"]
  })

  return {"sql_query": result}

write_query(
    {"question": "How many people over age 80 died in the accident?", "data_description": DB_DESCRIPTION, "plan": "I can find the number of people over age 80 who died by joining the table survivors with passengers and filtering by the Age and Survived columns."}
)

def execute_query(state):
  query = state["sql_query"]

  try:
    return {"sql_result": run_query(query)}
  except Exception as e:
    return {"sql_result": str(e)}

execute_query({'sql_query': 'SELECT COUNT(*)\nFROM survivors\nJOIN passengers ON survivors.PassengerId = passengers.PassengerId\nWHERE passengers.Age > 80 AND survivors.Survived = 0;'})

write_answer_prompt = PromptTemplate(
    template="""You are a database reading bot that can answer users' questions using information from a database. \n

    In the previous step, you have planned the query as follows: {plan},
    generated the query {sql_query}
    and retrieved the following data:
    {sql_result}

    Return a text answering the user's question using the provided data.

    Question: {question} \n""",
    input_variables=["question", "plan", "sql_query", "sql_result"],
)

write_answer_chain = write_answer_prompt | model | StrOutputParser()

def write_answer(state):
  result = write_answer_chain.invoke({
      "question": state["question"],
      "plan": state["plan"],
      "sql_query": state["sql_query"],
      "sql_result": state["sql_result"]
  })

  return {"answer": result}

write_answer(
    {
        "question": "How many people over age 80 died in the accident?",
        "plan": "I can determine how many people over age 80 died in the accident by joining the survivors table with the passengers table, filtering by Age greater than 80 and Survived equals 0.",
        "sql_query": "SELECT COUNT(*) \nFROM survivors \nJOIN passengers ON survivors.PassengerId = passengers.PassengerId \nWHERE passengers.Age > 80 AND survivors.Survived = 0;",
        "sql_result": "|    |   COUNT(*) |\n|---:|-----------:|\n|  0 |          0 |"
    }
)

cannot_answer_prompt = PromptTemplate(
    template="""You are a database reading bot that can answer users' questions using information from a database. \n

    You cannot answer the user's questions because of the following problem: {problem}.

    Explain the issue to the user and apologize for the inconvenience.

    Question: {question} \n""",
    input_variables=["question", "problem"],
)

cannot_answer_chain = cannot_answer_prompt | model | StrOutputParser()

def explain_no_answer(state):
  result = cannot_answer_chain.invoke({
      "problem": state["plan"],
      "question": state["question"]
  })

  return {"answer": result}

explain_no_answer(
    {
        "question": "How many French people were on board?",
        "plan": "I can't answer how many French people were on board because there is no information about passengers' nationalities."
    }
)

def check_if_can_answer(state):
  return state["can_answer"]

from langgraph.graph import StateGraph, END

workflow = StateGraph(WorkflowState)

workflow.add_node("check_if_can_answer_question", check_if_can_answer_question)
workflow.add_node("write_query", write_query)
workflow.add_node("execute_query", execute_query)
workflow.add_node("write_answer", write_answer)
workflow.add_node("explain_no_answer", explain_no_answer)

workflow.set_entry_point("check_if_can_answer_question")

workflow.add_conditional_edges("check_if_can_answer_question", check_if_can_answer, {
    True: "write_query",
    False: "explain_no_answer"
})

workflow.add_edge("write_query", "execute_query")
workflow.add_edge("execute_query", "write_answer")

workflow.add_edge("explain_no_answer", END)
workflow.add_edge("write_answer", END)

app = workflow.compile()

!pip install grandalf

app.get_graph().print_ascii()

inputs = {"question": "Count the number of people who boarded titanic grouped by the embarkment port"}
app.invoke(inputs)

inputs = {"question": "Count the number of French people"}
app.invoke(inputs)

inputs = {"question": "Count the number of people who boarded titanic grouped by the embarkment port"}
for output in app.stream(inputs, stream_mode="updates"):
  for function_name, updated_state in output.items():
    print(f"{function_name}: {updated_state}")

"""## ZADANIE 4"""

first_page = call_rest_api("GET", "/api/users?page=1")["data"]
second_page = call_rest_api("GET", "/api/users?page=2")["data"]
employees = first_page + second_page
employees = pd.DataFrame(employees)

con = sqlite3.connect("employees.db")
employees.to_sql("employees", con, index=False, if_exists="replace")

"""Z użyciem Langgraph przygotuj aplikację pozwalająca na wyszukiwanie informacji o pracownikach."""

def run_query(sql_query):
  con = sqlite3.connect("employees.db")
  try:
    response = pd.read_sql_query(sql_query, con)
    return response.to_markdown()
  except Exception as e:
    return str(e)
  finally:
    con.close()

def execute_query(state):
  query = state["sql_query"]
  try:
    return {"sql_result": run_query(query)}
  except Exception as e:
    return {"sql_result error": str(e)}

DB_DESCRIPTION = """
* "employees" Table:
Columns:
id: A unique identifier for each employee.
email: Email address of an employee.
first_name: First name of an employee.
last_name: Last name of an employee.
avatar: URL to the employee's avatar image. This cannot be shown ever.
"""

from typing_extensions import TypedDict
class WorkflowState(TypedDict):
  question: str
  plan: str
  can_answer: bool
  sql_query: str
  sql_result: str
  answer: str

write_query_prompt = PromptTemplate(
    template="""You are a database reading bot that can answer users' questions using information from a database. \n
    {data_description} \n\n
    In the previous step, you have prepared the following plan: {plan}
    Return an SQL query with no preamble or explanation. Don't include any markdown characters or quotation marks around the query.
    Question: {question} \n""",
    input_variables=["data_description", "question", "plan"],
)

write_query_chain = write_query_prompt | model | StrOutputParser()

def write_query(state):
  result = write_query_chain.invoke({
      "data_description": DB_DESCRIPTION,
      "question": state["question"],
      "plan": state["plan"]
  })
  return {"sql_query": result}

can_answer_prompt = PromptTemplate(
    template="""You are a database reading bot that can answer users' questions using information from a database. \n

    {data_description} \n\n

    Given the user's question, decide whether the question can be answered using the information in the database. \n\n

    Return a JSON with two keys, 'reasoning' and 'can_answer', and no preamble or explanation.
    Return one of the following JSON:

    {{"reasoning": "Our company name is Broken Tree.", "can_answer":true}}
{{"reasoning": "I can find the number of of employees by counting rows in the table", "can_answer":true}}
{{"reasoning": "I can find the employee by filtering by first_name and last_name or by ID or by email", "can_answer":true}}
{{"reasoning": "I cannot find oldest employee, because I don't have such data.", "can_answer":false}}
{{"reasoning": "I cannot provide employee avatar link, because it's a super confidential data.", "can_answer":false}}

Question: {question} \n
""",
input_variables=["data_description", "question"])

can_answer_chain = can_answer_prompt | model | JsonOutputParser()

def check_if_can_answer_question(state):
  result = can_answer_chain.invoke({"question": state["question"], "data_description": DB_DESCRIPTION})
  return {"plan": result["reasoning"], "can_answer": result["can_answer"]}

write_answer_prompt = PromptTemplate(
    template="""You are a database reading bot that can answer users' questions using information from a database. \n
    In the previous step, you have planned the query as follows: {plan},
    generated the query {sql_query}
    and retrieved the following data:
    {sql_result}
    Return a text answering the user's question using the provided data.
    Question: {question} \n""",
    input_variables=["question", "plan", "sql_query", "sql_result"],
)

write_answer_chain = write_answer_prompt | model | StrOutputParser()

def write_answer(state):
  result = write_answer_chain.invoke({
      "question": state["question"],
      "plan": state["plan"],
      "sql_query": state["sql_query"],
      "sql_result": state["sql_result"]
  })
  return {"answer": result}

cannot_answer_prompt = PromptTemplate(
    template="""You are a database reading bot that can answer users' questions using information from a database. \n
    You cannot answer the user's questions because of the following problem: {problem}.
    Explain the issue to the user and apologize for the inconvenience.
    Question: {question} \n""",
    input_variables=["question", "problem"],
)

cannot_answer_chain = cannot_answer_prompt | model | StrOutputParser()

def explain_no_answer(state):
  result = cannot_answer_chain.invoke({
      "problem": state["plan"],
      "question": state["question"]
  })
  return {"answer": result}

def check_if_can_answer(state):
  return state["can_answer"]

from langgraph.graph import StateGraph, END
workflow = StateGraph(WorkflowState)
workflow.add_node("check_if_can_answer_question", check_if_can_answer_question)
workflow.add_node("write_query", write_query)
workflow.add_node("execute_query", execute_query)
workflow.add_node("write_answer", write_answer)
workflow.add_node("explain_no_answer", explain_no_answer)
workflow.set_entry_point("check_if_can_answer_question")
workflow.add_conditional_edges("check_if_can_answer_question", check_if_can_answer, {
    True: "write_query",
    False: "explain_no_answer"
})
workflow.add_edge("write_query", "execute_query")
workflow.add_edge("execute_query", "write_answer")
workflow.add_edge("explain_no_answer", END)
workflow.add_edge("write_answer", END)
app = workflow.compile()

inputs = {"question": "Count the number of employees in the company."}
for output in app.stream(inputs, stream_mode="updates"):
  for function_name, updated_state in output.items():
    print(f"{function_name}: {updated_state}")

inputs = {"question": "Tell me all information about an employee with ID 1."}
for output in app.stream(inputs, stream_mode="updates"):
  for function_name, updated_state in output.items():
    print(f"{function_name}: {updated_state}")

inputs = {"question": "Show me avatars of all employees."}
for output in app.stream(inputs, stream_mode="updates"):
  for function_name, updated_state in output.items():
    print(f"{function_name}: {updated_state}")

"""## LangGraph -> Marvin"""

class CanAnswerQuestion(BaseModel):
  reasoning: str
  can_answer: bool


@marvin.fn
def check_if_question_can_be_answered(database_description: str, question: str) -> CanAnswerQuestion:
  """You are a database reading bot that can answer users' questions using information from a database. \n

    {database_description} \n\n

    Given the user's question, decide whether the question can be answered using the information in the database. \n\n

    Return a JSON with two keys, 'reasoning' and 'can_answer', and no preamble or explanation.
    Return one of the following JSON:

    "reasoning": "I can find the number of women who survived by joining the table survivors with passengers and filtering by the Sex and Survived columns", "can_answer":true
    "reasoning": "I can find the number of passengers in each class by grouping the data from tickets table by the Pclass column and counting the number of rows in each group", "can_answer":true
    "reasoning": "I can't answer how many people traveled in a cabin on the top deck because there is no deck level information.", "can_answer":false

    Question: {question} \n
    """

DB_DESCRIPTION = """
* "survivors" Table:
Columns:
PassengerId: A unique identifier for each passenger.
Survived: Indicates whether the passenger survived the sinking of the Titanic (typically coded as 1 for survived and 0 for did not survive).

* "tickets" Table:
Columns:
PassengerId: A unique identifier for each passenger, linking to the survivors table.
Ticket: The ticket number associated with each passenger.
Pclass: The passenger class; reflects the socio-economic status of the passenger (values: 1, 2, 3).
Fare: The amount of money paid for the ticket.
Cabin: The cabin number where the passenger stayed.
Embarked: The port at which the passenger embarked (C = Cherbourg; Q = Queenstown; S = Southampton).

* "passengers" Table:
Columns:
PassengerId: A unique identifier for each passenger, linking to the survivors table.
Name: The full name of the passenger.
Sex: The gender of the passenger.
Age: The age of the passenger at the time of the Titanic's voyage."""

check_if_question_can_be_answered(DB_DESCRIPTION, "How many people over age 80 died in the accident?")

@marvin.fn
def write_sql_query(database_description: str, question: str, plan: str) -> str:
  """You are a database reading bot that can answer users' questions using information from a database. \n

    {database_description} \n\n

    In the previous step, you have prepared the following plan: {plan}

    Return an SQL query with no preamble or explanation. Don't include any markdown characters or quotation marks around the query.

    Question: {question} \n"""

write_sql_query(DB_DESCRIPTION, "How many people over age 80 died in the accident?", "I can't answer how many people over age 80 died in the accident because there is no age information in the survivors table to determine who survived and who didn't.")

def run_query(sql_query):
  con = sqlite3.connect("titanic.db")
  try:
    response = pd.read_sql_query(sql_query, con)
    return response.to_markdown()
  except Exception as e:
    return str(e)
  finally:
    con.close()

@marvin.fn
def write_answer(question: str, plan: str, sql_query: str, sql_result: str) -> str:
  """You are a database reading bot that can answer users' questions using information from a database. \n

    In the previous step, you have planned the query as follows: {plan},
    generated the query {sql_query}
    and retrieved the following data:
    {sql_result}

    Return a text answering the user's question using the provided data.

    Question: {question} \n"""

write_answer(
  "How many people over age 80 died in the accident?",
  "I can determine how many people over age 80 died in the accident by joining the survivors table with the passengers table, filtering by Age greater than 80 and Survived equals 0.",
  "SELECT COUNT(*) \nFROM survivors \nJOIN passengers ON survivors.PassengerId = passengers.PassengerId \nWHERE passengers.Age > 80 AND survivors.Survived = 0;",
  "|    |   COUNT(*) |\n|---:|-----------:|\n|  0 |          0 |"
)

@marvin.fn
def explain_why_cannot_answer(question: str, problem_explanation: str) -> str:
  """You are a database reading bot that can answer users' questions using information from a database. \n

    You cannot answer the user's questions because of the following problem: {problem_explanation}.

    Explain the issue to the user and apologize for the inconvenience.

    Example:
    Question: How many French people were on board?
    Problem explanation: I can't answer how many French people were on board because there is no information about passengers' nationalities.

    Answer:
    I can't answer how many French people were on board because there is no information about passengers' nationalities. We apologize for the inconvenience.

    Question: {question} \n"""

def answer_user_question(question: str) -> str:
  can_answer = check_if_question_can_be_answered(DB_DESCRIPTION, question)

  if can_answer.can_answer:
    sql_query = write_sql_query(DB_DESCRIPTION, question, can_answer.reasoning)
    sql_result = run_query(sql_query)
    return write_answer(question, can_answer.reasoning, sql_query, sql_result)
  else:
    return explain_why_cannot_answer(question, can_answer.reasoning)

answer_user_question("How many people over age 80 died in the accident?")

answer_user_question("How many people traveled in a cabin on the top deck?")

"""## Zadanie 5

Wykonaj poprzednie zadanie przy użyciu Marvin.
"""

